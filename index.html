<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"/>
  <title>WebXR 2D→3D (Light ML • ONNX)</title>
  <style>
    html,body{height:100%;margin:0;background:#000;color:#ccc;font-family:system-ui,Arial,sans-serif}
    #hud{position:fixed;left:12px;top:12px;display:flex;gap:8px;z-index:3}
    .pill{background:#2b2b2b;border:1px solid #3a3a3a;border-radius:999px;color:#eee;padding:8px 12px;font-size:12px}
    .btn{cursor:pointer;user-select:none}
    .btn[aria-disabled="true"]{opacity:.5;cursor:not-allowed}
    #diag{position:fixed;left:12px;bottom:12px;font-size:12px;color:#9aa;white-space:pre-line;z-index:3;max-width:92vw}
    #overlay{position:fixed;inset:0;display:flex;flex-direction:column;gap:12px;align-items:center;justify-content:center;background:rgba(0,0,0,.55);z-index:4}
    .cta{background:#3a7bfd;color:#fff;border:none;border-radius:12px;padding:14px 20px;font-weight:700;cursor:pointer}
    video{position:fixed;left:-9999px;top:-9999px;width:1px;height:1px}
    canvas{display:block}
  </style>

  <!-- Three.js + VR button -->
  <script src="https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/webxr/VRButton.js"></script>
  <!-- HLS -->
  <script src="https://cdn.jsdelivr.net/npm/hls.js@1.5.8/dist/hls.min.js"></script>
  <!-- onnxruntime-web (WASM) -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
</head>
<body>
  <div id="hud">
    <span class="pill">Build v1.0 • Light-ML</span>
    <span id="enterVR" class="pill btn" aria-disabled="true">Enter VR</span>
    <span id="status" class="pill">idle</span>
  </div>
  <div id="diag"></div>

  <div id="overlay">
    <button id="play" class="cta">Enable autoplay & load</button>
    <div style="color:#aab">If the button is disabled in VR, click it on the 2D page first.</div>
  </div>

  <script>
  (async function(){
    // ---------- CONFIG ----------
    const STREAM_URL = "https://stream.mux.com/bQk01L9P9W5qE01cF53Bf9vYJ3PJLW6c.m3u8";
    const MODEL_URL  = "/models/midas_small_256.onnx"; // you added this already

    const DEPTH_IN_W = 256, DEPTH_IN_H = 256; // model expects square; we’ll sample to our grid
    const GRID_W = 128, GRID_H = 72;          // mesh density (keep modest on Quest)
    const RUN_EVERY_N_FRAMES = 6;             // throttle depth inference
    const DEPTH_SCALE_METERS = 0.30;          // parallax strength (0.2–0.35 safe)
    // ----------------------------

    const statusEl = document.getElementById('status');
    const diagEl   = document.getElementById('diag');
    const playBtn  = document.getElementById('play');
    const vrBtn    = document.getElementById('enterVR');
    const overlay  = document.getElementById('overlay');

    function setStatus(s){ statusEl.textContent = s; }
    function log(s){ diagEl.textContent = s; }

    // Video element (kept offscreen)
    const video = document.createElement('video');
    video.playsInline = true;
    video.muted = true; // allow autoplay after user gesture
    video.crossOrigin = 'anonymous';
    video.preload = 'auto';
    document.body.appendChild(video);

    // HLS hookup
    if (Hls.isSupported()) {
      const hls = new Hls({lowLatencyMode:true, backBufferLength:30});
      hls.loadSource(STREAM_URL);
      hls.attachMedia(video);
      hls.on(Hls.Events.ERROR, (e,d)=>log(`[HLS] ${d?.details||'error'}`));
    } else {
      video.src = STREAM_URL;
    }

    // Three.js scene
    const renderer = new THREE.WebGLRenderer({antialias:true, alpha:false});
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(Math.min(devicePixelRatio, 1.5));
    renderer.xr.enabled = true;
    document.body.appendChild(renderer.domElement);

    const scene = new THREE.Scene();
    scene.background = new THREE.Color(0x000000);

    const camera = new THREE.PerspectiveCamera(70, window.innerWidth/window.innerHeight, 0.01, 100);
    camera.position.set(0,0,0);

    // Dark room so you don’t see “nothing”
    const room = new THREE.Mesh(
      new THREE.BoxGeometry(10,6,10),
      new THREE.MeshBasicMaterial({color:0x050505, side:THREE.BackSide})
    );
    scene.add(room);

    // Video texture
    const tex = new THREE.VideoTexture(video);
    tex.minFilter = THREE.LinearFilter; tex.magFilter = THREE.LinearFilter;

    // Panel size (16:9-ish)
    const PLANE_W = 3.2, PLANE_H = 1.8;

    // Dense mesh that we’ll warp using depth
    const geo = new THREE.PlaneGeometry(PLANE_W, PLANE_H, GRID_W-1, GRID_H-1);
    const mat = new THREE.MeshBasicMaterial({map:tex, toneMapped:false});
    const depthMesh = new THREE.Mesh(geo, mat);
    depthMesh.position.set(0, 0, -3.0);
    scene.add(depthMesh);

    // Resize handler
    addEventListener('resize', ()=>{
      renderer.setSize(innerWidth, innerHeight);
      camera.aspect = innerWidth/innerHeight;
      camera.updateProjectionMatrix();
    });

    // Hidden native VR button
    const nativeVRBtn = THREE.VRButton.createButton(renderer);
    nativeVRBtn.style.display = 'none';
    document.body.appendChild(nativeVRBtn);

    vrBtn.addEventListener('click', ()=> nativeVRBtn.click());

    // ---- onnxruntime-web (WASM) init ----
    setStatus('loading model…');
    let session = null;
    try {
      ort.env.wasm.numThreads = 1;               // stable on Quest
      ort.env.wasm.simd = true;                  // enable SIMD if available
      session = await ort.InferenceSession.create(MODEL_URL, {
        executionProviders: ['wasm']            // wasm for broad compatibility
      });
      setStatus('model ready');
    } catch (e){
      setStatus('model failed — showing flat video');
      log(String(e));
    }

    // Scratch canvas to feed frames to model
    const c = document.createElement('canvas');
    c.width = DEPTH_IN_W; c.height = DEPTH_IN_H;
    const cx = c.getContext('2d', {willReadFrequently:true});

    // Helpers to build ONNX tensor (NCHW, float32, normalized 0..1)
    function toCHWFloat32(imgData){
      const {data,width,height} = imgData;
      const chw = new Float32Array(1*3*height*width);
      let r=0,g=height*width,b=2*height*width;
      for (let i=0,px=0; px<width*height; px++, i+=4){
        const R = data[i]/255, G = data[i+1]/255, B = data[i+2]/255;
        chw[r++] = R; chw[g++] = G; chw[b++] = B;
      }
      return new ort.Tensor('float32', chw, [1,3,height,width]);
    }

    // Run depth inference (throttled by RUN_EVERY_N_FRAMES)
    let frame = 0;
    const posAttr = geo.attributes.position;
    const verts = posAttr.array;

    function updateMeshZFromDepth(depthData, outW, outH){
      // Normalize depth to 0..1 and invert (near=1)
      let min=Infinity, max=-Infinity;
      for (let i=0;i<depthData.length;i++){ const v=depthData[i]; if(v<min)min=v; if(v>max)max=v; }
      const range = (max-min)||1;

      for (let gy=0; gy<GRID_H; gy++){
        const sy = Math.floor(gy*(outH-1)/(GRID_H-1));
        for (let gx=0; gx<GRID_W; gx++){
          const sx = Math.floor(gx*(outW-1)/(GRID_W-1));
          const di = sy*outW + sx;
          const dNorm = 1 - ((depthData[di]-min)/range); // near=1
          const i3 = (gy*GRID_W + gx)*3;
          verts[i3+2] = -dNorm * DEPTH_SCALE_METERS;
        }
      }
      posAttr.needsUpdate = true;
    }

    // UI: enable video then VR
    playBtn.addEventListener('click', async ()=>{
      try{
        await video.play();
        overlay.remove();
        vrBtn.setAttribute('aria-disabled','false');
        setStatus('playing… enter VR when ready');
      }catch(e){
        setStatus('tap again to allow playback');
      }
    });

    // Render / inference loop
    renderer.setAnimationLoop(async ()=>{
      if (!video.paused && !video.ended && session){
        frame++;
        if (frame % RUN_EVERY_N_FRAMES === 0){
          try{
            // draw current video frame into square input
            cx.drawImage(video, 0, 0, DEPTH_IN_W, DEPTH_IN_H);
            const img = cx.getImageData(0, 0, DEPTH_IN_W, DEPTH_IN_H);
            const input = toCHWFloat32(img);
            const feeds = { "input": input }; // MiDaS small ONNX often uses "input"—but model IO names vary

            // If your model expects specific input/output names, adjust here:
            const outputMap = await session.run(feeds);
            // Pick first output tensor
            const outName = Object.keys(outputMap)[0];
            const out = outputMap[outName];
            const outW = out.dims[out.dims.length-1];
            const outH = out.dims[out.dims.length-2];
            const depthData = out.data; // Float32Array length outW*outH

            updateMeshZFromDepth(depthData, outW, outH);
          } catch(e){
            // If a single frame fails, keep going with last geometry
            log(`depth frame err: ${e?.message||e}`);
          }
        }
      }
      renderer.render(scene, camera);
    });
  })();
  </script>
</body>
</html>