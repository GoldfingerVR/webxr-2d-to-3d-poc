<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"/>
<title>WebXR 2D→3D POC — MiDaS Small, VR Depth</title>
<style>
  html,body{margin:0;height:100%;background:#000;overflow:hidden;font-family:system-ui,-apple-system,Segoe UI,Roboto,sans-serif}
  #hud{position:fixed;z-index:3;left:16px;top:16px;display:flex;gap:8px;align-items:center}
  .pill{background:#303030;color:#e0e0e0;border-radius:999px;padding:6px 10px;font-size:12px;border:1px solid #444}
  .btn{cursor:pointer;user-select:none}
  .btn[aria-disabled="true"]{opacity:.45;cursor:not-allowed}
  #status{opacity:.85}
  #panel{position:fixed;right:16px;top:16px;display:flex;align-items:center;gap:8px;z-index:3}
  #panel label{color:#ddd;font-size:13px;display:flex;align-items:center;gap:6px}
  #overlay{
    position:fixed;inset:0;display:none;flex-direction:column;gap:10px;
    align-items:center;justify-content:center;z-index:4;background:rgba(0,0,0,.6)
  }
  #dlbox{width:min(520px,86vw);background:#151515;border:1px solid #333;border-radius:12px;padding:16px}
  #bar{height:10px;background:#2a2a2a;border-radius:999px;overflow:hidden}
  #bar>span{display:block;height:100%;width:0%;background:#00a86b}
  #msg{color:#ccc;font-size:13px;margin-top:8px}
  #diag{position:fixed;left:12px;bottom:12px;color:#888;font-size:11px;white-space:pre-line;z-index:3;max-width:92vw}
  video{position:fixed;left:-9999px;top:-9999px;width:1px;height:1px}
  canvas{display:block}
</style>
</head>
<body>
  <!-- Left HUD -->
  <div id="hud">
    <span class="pill">Build v0.20</span>
    <span id="enterVR" class="pill btn" aria-disabled="true">Enter VR</span>
    <span id="status" class="pill">idle</span>
  </div>

  <!-- Right controls -->
  <div id="panel">
    <label class="pill"><input id="autoplayCk" type="checkbox"/> Enable Autoplay</label>
    <span class="pill">Video: sample MP4</span>
  </div>

  <!-- Model download overlay -->
  <div id="overlay">
    <div id="dlbox">
      <div style="color:#e6e6e6;font-weight:700;margin-bottom:6px">Preparing 3D Depth (MiDaS Small)</div>
      <div id="bar"><span></span></div>
      <div id="msg">Downloading model…</div>
    </div>
  </div>

  <!-- Hidden media source -->
  <video id="vid" playsinline muted crossorigin="anonymous"></video>
  <!-- Hidden canvases for preprocessing -->
  <canvas id="cPre" width="256" height="256" style="position:fixed;left:-9999px;top:-9999px"></canvas>

  <div id="diag"></div>

  <script type="module">
    import * as THREE from 'https://unpkg.com/three@0.160.0/build/three.module.js';
    // ONNX Runtime Web (WASM backend)
    import 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.18.0/dist/ort.min.js';

    // --------- CONFIG ----------
    const STREAM_URL = 'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4';
    // Your ONNX model is in repo at models/midas_v21_small_256.onnx (no placeholders)
    const MODEL_URL  = 'models/midas_v21_small_256.onnx';
    // Parallax strength (meters). Start small to avoid eye strain.
    const PARALLAX   = 0.035;
    // Infer every Nth frame and reuse between runs
    const INFER_EVERY = 2; // run depth ~30fps video → ~15fps depth

    // ---------- DOM ----------
    const video    = document.getElementById('vid');
    const enterVR  = document.getElementById('enterVR');
    const autoplay = document.getElementById('autoplayCk');
    const overlay  = document.getElementById('overlay');
    const barInner = document.querySelector('#bar>span');
    const msg      = document.getElementById('msg');
    const statusE  = document.getElementById('status');
    const diag     = document.getElementById('diag');
    const cPre     = document.getElementById('cPre');
    const ctxPre   = cPre.getContext('2d', { willReadFrequently:true });

    // ---------- Three / XR ----------
    let renderer, scene, camera;
    let videoTex, depthTex, shaderLeft, shaderRight, quadLeft, quadRight, monoMesh;
    let sessionXR = null;
    let readyStereo = false;

    // Depth state
    let ortSession = null;
    let frameCount = 0;
    let lastDepthImageData = null; // Uint8ClampedArray for grayscale
    let depthW = 256, depthH = 256;

    initThree();
    createMonoPlane();
    wireUI();
    animate();
    probeXR();

    // UI logic
    function wireUI(){
      autoplay.addEventListener('change', async ()=>{
        if (autoplay.checked){
          // Start playback muted to satisfy autoplay policies
          await startPlayback();
          // Enable Enter VR after user consent
          enterVR.setAttribute('aria-disabled', 'false');
        }else{
          // Stop playback + lock VR
          video.pause();
          enterVR.setAttribute('aria-disabled', 'true');
          status('autoplay disabled');
        }
      });

      enterVR.addEventListener('click', async ()=>{
        if (enterVR.getAttribute('aria-disabled') === 'true') return;
        await startXR();
      });
    }

    async function startPlayback(){
      try{
        video.src = STREAM_URL;
        await video.play();
        video.muted = false;
        bindVideoTexture();
        status('playing (mono)');
      }catch(e){
        status('autoplay blocked');
        console.warn(e);
      }
    }

    function bindVideoTexture(){
      if (!videoTex){
        videoTex = new THREE.VideoTexture(video);
        videoTex.colorSpace = THREE.SRGBColorSpace;
        videoTex.minFilter = THREE.LinearFilter;
        videoTex.magFilter = THREE.LinearFilter;
        videoTex.generateMipmaps = false;
      }
      monoMesh.material.map = videoTex;
      monoMesh.material.needsUpdate = true;

      video.addEventListener('loadedmetadata', ()=>{
        const w = video.videoWidth || 16, h = video.videoHeight || 9;
        const aspect = w / Math.max(1,h);
        const width = 2.8, height = width / aspect;

        monoMesh.geometry.dispose();
        monoMesh.geometry = new THREE.PlaneGeometry(width, height);
      }, { once:true });
    }

    function createMonoPlane(){
      const geom = new THREE.PlaneGeometry(2.8, 1.575);
      const mat  = new THREE.MeshBasicMaterial({ color:0xffffff, transparent:false });
      monoMesh = new THREE.Mesh(geom, mat);
      monoMesh.position.set(0, 1.6, -2.0);
      scene.add(monoMesh);
    }

    // Build stereo quads with depth-aware shader
    function buildStereoWithShader(){
      if (!videoTex || !lastDepthImageData) return;

      monoMesh.visible = false;

      // Depth texture from last inference
      const depthData = new Uint8Array(lastDepthImageData.buffer.slice(0));
      depthTex = new THREE.DataTexture(depthData, depthW, depthH, THREE.LuminanceFormat);
      depthTex.needsUpdate = true;

      const width  = monoMesh.geometry.parameters.width;
      const height = monoMesh.geometry.parameters.height;
      const geom = new THREE.PlaneGeometry(width, height);

      const makeMat = (eyeSign) => new THREE.ShaderMaterial({
        uniforms: {
          uVideo: { value: videoTex },
          uDepth: { value: depthTex },
          uParallax: { value: PARALLAX * (eyeSign>0 ? 1 : -1) },
          uAspectVideo: { value: width/height }
        },
        vertexShader: `
          varying vec2 vUv;
          void main(){
            vUv = uv;
            gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
          }
        `,
        fragmentShader: `
          precision highp float;
          uniform sampler2D uVideo;
          uniform sampler2D uDepth;
          uniform float uParallax;
          varying vec2 vUv;
          // Depth map is grayscale in [0,1]; treat 0 = far, 1 = near
          void main(){
            float d = texture2D(uDepth, vUv).r;       // 0..1
            // Center disparity so mid-depth ~0
            float disp = (0.5 - d) * uParallax;       // near moves opposite sign
            vec2 uv = vec2(vUv.x + disp, vUv.y);
            vec4 col = texture2D(uVideo, uv);
            gl_FragColor = col;
          }
        `,
        depthTest: false,
        depthWrite: false
      });

      shaderLeft  = makeMat(-1);
      shaderRight = makeMat(+1);

      quadLeft  = new THREE.Mesh(geom.clone(), shaderLeft);
      quadRight = new THREE.Mesh(geom.clone(), shaderRight);

      quadLeft.position.copy(monoMesh.position);
      quadRight.position.copy(monoMesh.position);

      // Eye masks
      quadLeft.layers.set(1);
      quadRight.layers.set(2);

      scene.add(quadLeft);
      scene.add(quadRight);
      readyStereo = true;
    }

    async function startXR(){
      if (!navigator.xr) { status('WebXR not available'); return; }
      try{
        sessionXR = await navigator.xr.requestSession('immersive-vr', {
          requiredFeatures:['local-floor'],
          optionalFeatures:['hand-tracking']
        });
        renderer.xr.setSession(sessionXR);
        status('VR started');

        // Show model progress and load + init
        overlay.style.display = 'flex';
        const modelBytes = await fetchWithProgress(MODEL_URL, (p) => {
          barInner.style.width = (p*100).toFixed(1)+'%';
          msg.textContent = 'Downloading model… ' + (p*100).toFixed(1) + '%';
        });
        msg.textContent = 'Initializing depth engine…';
        await initOrt(modelBytes);
        msg.textContent = 'Model ready — enabling 3D.';
        setTimeout(()=>{ overlay.style.display='none'; }, 400);

        // Kick first inference immediately
        await runDepthOnce();
        buildStereoWithShader();
        status('VR 3D active');
        sessionXR.addEventListener('end', ()=>{
          status('VR ended');
          // fall back to mono
          if (quadLeft)  quadLeft.visible  = false;
          if (quadRight) quadRight.visible = false;
          monoMesh.visible = true;
        });
      }catch(e){
        status('VR failed');
        console.error(e);
      }
    }

    async function initOrt(modelArrayBuffer){
      // Prefer WASM backend
      ort.env.wasm.wasmPaths = 'https://cdn.jsdelivr.net/npm/onnxruntime-web@1.18.0/dist/';
      ortSession = await ort.InferenceSession.create(modelArrayBuffer, {
        executionProviders: ['wasm'],
        graphOptimizationLevel: 'all'
      });
    }

    // Download with progress (works on GitHub Pages if Content-Length is present)
    async function fetchWithProgress(url, onProgress){
      const res = await fetch(url);
      const len = +res.headers.get('Content-Length') || 0;
      const reader = res.body.getReader();
      let received = 0;
      const chunks = [];
      for(;;){
        const { done, value } = await reader.read();
        if (done) break;
        chunks.push(value);
        received += value.byteLength;
        if (len && onProgress) onProgress(received/len);
      }
      const blob = new Blob(chunks);
      return await blob.arrayBuffer();
    }

    // Depth inference for current video frame → updates lastDepthImageData
    async function runDepthOnce(){
      // Draw video frame into 256x256 canvas
      ctxPre.drawImage(video, 0, 0, 256, 256);
      const img = ctxPre.getImageData(0,0,256,256);
      // Convert RGBA to float32 RGB normalized
      const src = img.data;
      const chw = new Float32Array(256*256*3);
      let p = 0;
      for (let i=0;i<src.length;i+=4){
        chw[p++] = src[i]   / 255; // R
        chw[p++] = src[i+1] / 255; // G
        chw[p++] = src[i+2] / 255; // B
      }
      // MiDaS small expects NCHW [1,3,256,256]
      const tensor = new ort.Tensor('float32', chw, [1,3,256,256]);
      const out = await ortSession.run({ input: tensor }); // input name 'input' works for many MiDaS small ONNX exports
      // Get first (and only) output
      const outName = Object.keys(out)[0];
      const depth = out[outName].data; // Float32Array, shape [1,1,H,W] or [1,H,W]
      // Convert to 0..255 grayscale
      // Find min/max
      let mn = Infinity, mx = -Infinity;
      for (let i=0;i<depth.length;i++){ const v = depth[i]; if (v<mn) mn=v; if (v>mx) mx=v; }
      const range = Math.max(1e-6, mx-mn);
      const N = depth.length;
      const gray = new Uint8ClampedArray(N);
      for (let i=0;i<N;i++){
        const n = (depth[i]-mn)/range; // 0..1
        gray[i] = Math.max(0, Math.min(255, Math.round(n*255)));
      }
      // Store for texture update
      lastDepthImageData = gray;
      depthW = 256; depthH = 256;
      if (depthTex){
        depthTex.image.data = new Uint8Array(gray.buffer.slice(0));
        depthTex.needsUpdate = true;
      }
    }

    // ---------- Three boilerplate ----------
    function initThree(){
      renderer = new THREE.WebGLRenderer({ antialias:true, alpha:false });
      renderer.setPixelRatio(Math.min(2, window.devicePixelRatio));
      renderer.setSize(window.innerWidth, window.innerHeight);
      renderer.xr.enabled = true;
      document.body.appendChild(renderer.domElement);

      scene = new THREE.Scene();
      scene.background = new THREE.Color(0x000000);

      camera = new THREE.PerspectiveCamera(70, window.innerWidth/window.innerHeight, 0.01, 100);
      camera.position.set(0, 1.6, 0);

      scene.add(new THREE.AmbientLight(0xffffff, 0.7));

      const floor = new THREE.Mesh(
        new THREE.CircleGeometry(3.5, 64),
        new THREE.MeshBasicMaterial({ color:0x101010 })
      );
      floor.rotation.x = -Math.PI/2;
      floor.position.y = 0;
      scene.add(floor);

      window.addEventListener('resize', ()=>{
        camera.aspect = window.innerWidth/window.innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(window.innerWidth, window.innerHeight);
      });
    }

    async function probeXR(){
      const httpsOK = (location.protocol === 'https:') || (location.hostname === 'localhost');
      const xrOK = !!navigator.xr;
      let supported = false;
      try { supported = xrOK ? await navigator.xr.isSessionSupported('immersive-vr') : false; }
      catch(e) {}
      enterVR.setAttribute('aria-disabled', (httpsOK && supported) ? 'false' : 'true');
      diag.textContent =
        `https/localhost: ${httpsOK}\n`+
        `navigator.xr: ${xrOK}\n`+
        `immersive-vr supported: ${supported}`;
    }

    function status(m){ statusE.textContent = m; }

    function animate(){
      renderer.setAnimationLoop(async ()=>{
        if (videoTex && !video.paused && !video.ended) {
          videoTex.needsUpdate = true;
        }
        // Run depth periodically once model is ready
        if (sessionXR && ortSession && (frameCount++ % INFER_EVERY === 0)){
          try{
            await runDepthOnce();
          }catch(e){ /* keep rendering even if a frame fails */ }
        }
        renderer.render(scene, camera);
      });
    }
  </script>

</body>
</html>